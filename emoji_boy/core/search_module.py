"""
æœç´¢æ¨¡å— - å¢å¼ºemoji_boyå¯¹æ•°å­—ä¸–ç•Œçš„æ„ŸçŸ¥èƒ½åŠ›
"""

import webbrowser
import requests
import json
import urllib.parse
from typing import Optional, Dict, Any, List
import time
import re
from bs4 import BeautifulSoup
import config


class SearchModule:
    """æœç´¢æ¨¡å— - æä¾›å¤šç§æœç´¢åŠŸèƒ½"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœç´¢æ¨¡å—"""
        self.search_engines = {
            'google': 'https://www.google.com/search?q={}',
            'bing': 'https://www.bing.com/search?q={}',
            'baidu': 'https://www.baidu.com/s?wd={}',
            'duckduckgo': 'https://duckduckgo.com/?q={}'
        }
        
        # é»˜è®¤æœç´¢å¼•æ“
        self.default_engine = 'google'
        
        # æœç´¢å†å²
        self.search_history = []
        self.max_history = 20
        
        # è¯·æ±‚é…ç½®
        self.timeout = 10
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def web_search(self, query: str, engine: str = None, crawl_results: bool = True) -> Dict[str, Any]:
        """
        æ‰§è¡Œç½‘é¡µæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            engine: æœç´¢å¼•æ“ ('google', 'bing', 'baidu', 'duckduckgo')
            crawl_results: æ˜¯å¦çˆ¬å–æœç´¢ç»“æœå†…å®¹
            
        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        try:
            engine = engine or self.default_engine
            if engine not in self.search_engines:
                engine = self.default_engine
            
            # æ„å»ºæœç´¢URL
            search_url = self.search_engines[engine].format(urllib.parse.quote(query))
            
            # è®°å½•æœç´¢å†å²
            self._add_to_history(query, engine, search_url)
            
            # åœ¨é»˜è®¤æµè§ˆå™¨ä¸­æ‰“å¼€
            webbrowser.open(search_url)
            
            # çˆ¬å–æœç´¢ç»“æœå†…å®¹
            crawled_content = None
            if crawl_results:
                crawled_content = self._crawl_search_results(query, engine)
            
            return {
                'success': True,
                'query': query,
                'engine': engine,
                'url': search_url,
                'crawled_content': crawled_content,
                'message': f'ğŸ” å·²åœ¨{engine}ä¸­æœç´¢: {query}' + (f'\nğŸ“„ å·²è·å–{len(crawled_content) if crawled_content else 0}æ¡æœç´¢ç»“æœ' if crawled_content else '')
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'message': f'âŒ æœç´¢å¤±è´¥: {str(e)}'
            }
    
    def smart_search(self, query: str, crawl_results: bool = True) -> Dict[str, Any]:
        """
        æ™ºèƒ½æœç´¢ - æ ¹æ®æŸ¥è¯¢å†…å®¹é€‰æ‹©åˆé€‚çš„æœç´¢æ–¹å¼
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            crawl_results: æ˜¯å¦çˆ¬å–æœç´¢ç»“æœå†…å®¹
            
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            # åˆ†ææŸ¥è¯¢ç±»å‹
            query_type = self._analyze_query_type(query)
            
            if query_type == 'knowledge':
                return self._knowledge_search(query, crawl_results)
            elif query_type == 'news':
                return self._news_search(query, crawl_results)
            elif query_type == 'image':
                return self._image_search(query, crawl_results)
            else:
                return self.web_search(query, crawl_results=crawl_results)
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'message': f'âŒ æ™ºèƒ½æœç´¢å¤±è´¥: {str(e)}'
            }
    
    def _analyze_query_type(self, query: str) -> str:
        """åˆ†ææŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        # çŸ¥è¯†ç±»æŸ¥è¯¢å…³é”®è¯
        knowledge_keywords = ['æ˜¯ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å®šä¹‰', 'æ¦‚å¿µ', 'åŸç†', 'æ–¹æ³•']
        if any(keyword in query_lower for keyword in knowledge_keywords):
            return 'knowledge'
        
        # æ–°é—»ç±»æŸ¥è¯¢å…³é”®è¯
        news_keywords = ['æ–°é—»', 'æœ€æ–°', 'ä»Šå¤©', 'æ˜¨å¤©', 'å‘ç”Ÿ', 'äº‹ä»¶', 'æŠ¥é“']
        if any(keyword in query_lower for keyword in news_keywords):
            return 'news'
        
        # å›¾ç‰‡ç±»æŸ¥è¯¢å…³é”®è¯
        image_keywords = ['å›¾ç‰‡', 'ç…§ç‰‡', 'å›¾åƒ', 'å›¾æ ‡', 'logo', 'å£çº¸']
        if any(keyword in query_lower for keyword in image_keywords):
            return 'image'
        
        return 'general'
    
    def _knowledge_search(self, query: str, crawl_results: bool = True) -> Dict[str, Any]:
        """çŸ¥è¯†æœç´¢"""
        # ä½¿ç”¨ç™¾åº¦ç™¾ç§‘æˆ–ç»´åŸºç™¾ç§‘æœç´¢
        search_url = f"https://www.baidu.com/s?wd={urllib.parse.quote(query + ' ç™¾ç§‘')}"
        webbrowser.open(search_url)
        
        # çˆ¬å–æœç´¢ç»“æœ
        crawled_content = None
        if crawl_results:
            crawled_content = self._crawl_search_results(query + ' ç™¾ç§‘', 'baidu')
        
        return {
            'success': True,
            'query': query,
            'type': 'knowledge',
            'url': search_url,
            'crawled_content': crawled_content,
            'message': f'ğŸ“š çŸ¥è¯†æœç´¢: {query}' + (f'\nğŸ“„ å·²è·å–{len(crawled_content) if crawled_content else 0}æ¡ç™¾ç§‘ç»“æœ' if crawled_content else '')
        }
    
    def _news_search(self, query: str, crawl_results: bool = True) -> Dict[str, Any]:
        """æ–°é—»æœç´¢"""
        search_url = f"https://www.google.com/search?q={urllib.parse.quote(query + ' æ–°é—»')}&tbm=nws"
        webbrowser.open(search_url)
        
        # çˆ¬å–æœç´¢ç»“æœ
        crawled_content = None
        if crawl_results:
            crawled_content = self._crawl_search_results(query + ' æ–°é—»', 'google')
        
        return {
            'success': True,
            'query': query,
            'type': 'news',
            'url': search_url,
            'crawled_content': crawled_content,
            'message': f'ğŸ“° æ–°é—»æœç´¢: {query}' + (f'\nğŸ“„ å·²è·å–{len(crawled_content) if crawled_content else 0}æ¡æ–°é—»ç»“æœ' if crawled_content else '')
        }
    
    def _image_search(self, query: str, crawl_results: bool = True) -> Dict[str, Any]:
        """å›¾ç‰‡æœç´¢"""
        search_url = f"https://www.google.com/search?q={urllib.parse.quote(query)}&tbm=isch"
        webbrowser.open(search_url)
        
        # å›¾ç‰‡æœç´¢é€šå¸¸ä¸éœ€è¦çˆ¬å–å†…å®¹ï¼Œä½†å¯ä»¥è·å–å›¾ç‰‡é“¾æ¥
        crawled_content = None
        if crawl_results:
            # å¯¹äºå›¾ç‰‡æœç´¢ï¼Œæˆ‘ä»¬å¯ä»¥è·å–å›¾ç‰‡çš„é“¾æ¥ä¿¡æ¯
            try:
                response = requests.get(search_url, headers=self.headers, timeout=self.timeout)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾å›¾ç‰‡é“¾æ¥
                img_links = soup.find_all('img')
                crawled_content = []
                
                for img in img_links[:5]:  # é™åˆ¶å‰5å¼ å›¾ç‰‡
                    src = img.get('src', '')
                    alt = img.get('alt', '')
                    if src and not src.startswith('data:'):
                        crawled_content.append({
                            'title': alt or f'å›¾ç‰‡ {len(crawled_content) + 1}',
                            'link': src,
                            'snippet': f'å›¾ç‰‡é“¾æ¥: {src}'
                        })
                        
            except Exception as e:
                print(f"âš ï¸ è·å–å›¾ç‰‡ä¿¡æ¯å¤±è´¥: {e}")
        
        return {
            'success': True,
            'query': query,
            'type': 'image',
            'url': search_url,
            'crawled_content': crawled_content,
            'message': f'ğŸ–¼ï¸ å›¾ç‰‡æœç´¢: {query}' + (f'\nğŸ“„ å·²è·å–{len(crawled_content) if crawled_content else 0}å¼ å›¾ç‰‡' if crawled_content else '')
        }
    
    def search_with_context(self, query: str, context: str = "") -> Dict[str, Any]:
        """
        å¸¦ä¸Šä¸‹æ–‡çš„æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œå°†å…¶æ·»åŠ åˆ°æŸ¥è¯¢ä¸­
            if context:
                enhanced_query = f"{context} {query}"
            else:
                enhanced_query = query
            
            return self.smart_search(enhanced_query)
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'message': f'âŒ ä¸Šä¸‹æ–‡æœç´¢å¤±è´¥: {str(e)}'
            }
    
    def get_search_suggestions(self, query: str) -> List[str]:
        """
        è·å–æœç´¢å»ºè®®
        
        Args:
            query: æŸ¥è¯¢è¯
            
        Returns:
            å»ºè®®åˆ—è¡¨
        """
        try:
            # è¿™é‡Œå¯ä»¥é›†æˆæœç´¢å»ºè®®API
            # ç›®å‰è¿”å›åŸºäºå†å²çš„åŸºæœ¬å»ºè®®
            suggestions = []
            
            # ä»å†å²è®°å½•ä¸­æŸ¥æ‰¾ç›¸å…³å»ºè®®
            for hist in self.search_history:
                if query.lower() in hist['query'].lower():
                    suggestions.append(hist['query'])
            
            # æ·»åŠ ä¸€äº›é€šç”¨å»ºè®®
            if 'å¦‚ä½•' in query:
                suggestions.extend([f"{query} æ­¥éª¤", f"{query} æ–¹æ³•", f"{query} æ•™ç¨‹"])
            elif 'æ˜¯ä»€ä¹ˆ' in query:
                suggestions.extend([f"{query} å®šä¹‰", f"{query} æ¦‚å¿µ", f"{query} è§£é‡Š"])
            
            return list(set(suggestions))[:5]  # å»é‡å¹¶é™åˆ¶æ•°é‡
            
        except Exception as e:
            print(f"âš ï¸ è·å–æœç´¢å»ºè®®å¤±è´¥: {e}")
            return []
    
    def _add_to_history(self, query: str, engine: str, url: str):
        """æ·»åŠ åˆ°æœç´¢å†å²"""
        history_item = {
            'query': query,
            'engine': engine,
            'url': url,
            'timestamp': time.time()
        }
        
        self.search_history.append(history_item)
        
        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self.search_history) > self.max_history:
            self.search_history = self.search_history[-self.max_history:]
    
    def get_search_history(self) -> List[Dict[str, Any]]:
        """è·å–æœç´¢å†å²"""
        return self.search_history.copy()
    
    def clear_search_history(self):
        """æ¸…ç©ºæœç´¢å†å²"""
        self.search_history.clear()
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æœç´¢æ¨¡å—çŠ¶æ€"""
        return {
            'available_engines': list(self.search_engines.keys()),
            'default_engine': self.default_engine,
            'history_count': len(self.search_history),
            'max_history': self.max_history
        }
    
    def set_default_engine(self, engine: str):
        """è®¾ç½®é»˜è®¤æœç´¢å¼•æ“"""
        if engine in self.search_engines:
            self.default_engine = engine
            return True
        return False
    
    def _crawl_search_results(self, query: str, engine: str) -> List[Dict[str, str]]:
        """
        çˆ¬å–æœç´¢ç»“æœå†…å®¹
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            engine: æœç´¢å¼•æ“
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # æ„å»ºæœç´¢URL
            search_url = self.search_engines[engine].format(urllib.parse.quote(query))
            
            # å‘é€HTTPè¯·æ±‚
            response = requests.get(search_url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()
            
            # è§£æHTMLå†…å®¹
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ ¹æ®æœç´¢å¼•æ“æå–æœç´¢ç»“æœ
            if engine == 'google':
                return self._extract_google_results(soup)
            elif engine == 'bing':
                return self._extract_bing_results(soup)
            elif engine == 'baidu':
                return self._extract_baidu_results(soup)
            else:
                return self._extract_generic_results(soup)
                
        except Exception as e:
            print(f"âš ï¸ çˆ¬å–æœç´¢ç»“æœå¤±è´¥: {e}")
            return []
    
    def _extract_google_results(self, soup) -> List[Dict[str, str]]:
        """æå–Googleæœç´¢ç»“æœ"""
        results = []
        
        try:
            # æŸ¥æ‰¾æœç´¢ç»“æœå®¹å™¨
            search_results = soup.find_all('div', class_='g')
            
            for result in search_results[:5]:  # é™åˆ¶å‰5ä¸ªç»“æœ
                try:
                    # æå–æ ‡é¢˜
                    title_elem = result.find('h3')
                    title = title_elem.get_text().strip() if title_elem else ""
                    
                    # æå–é“¾æ¥
                    link_elem = result.find('a')
                    link = link_elem.get('href') if link_elem else ""
                    
                    # æå–æ‘˜è¦
                    snippet_elem = result.find('div', class_='VwiC3b')
                    snippet = snippet_elem.get_text().strip() if snippet_elem else ""
                    
                    if title and link:
                        results.append({
                            'title': title,
                            'link': link,
                            'snippet': snippet
                        })
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ æå–Googleç»“æœå¤±è´¥: {e}")
        
        return results
    
    def _extract_bing_results(self, soup) -> List[Dict[str, str]]:
        """æå–Bingæœç´¢ç»“æœ"""
        results = []
        
        try:
            # æŸ¥æ‰¾æœç´¢ç»“æœå®¹å™¨
            search_results = soup.find_all('li', class_='b_algo')
            
            for result in search_results[:5]:  # é™åˆ¶å‰5ä¸ªç»“æœ
                try:
                    # æå–æ ‡é¢˜
                    title_elem = result.find('h2')
                    title = title_elem.get_text().strip() if title_elem else ""
                    
                    # æå–é“¾æ¥
                    link_elem = result.find('a')
                    link = link_elem.get('href') if link_elem else ""
                    
                    # æå–æ‘˜è¦
                    snippet_elem = result.find('p')
                    snippet = snippet_elem.get_text().strip() if snippet_elem else ""
                    
                    if title and link:
                        results.append({
                            'title': title,
                            'link': link,
                            'snippet': snippet
                        })
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ æå–Bingç»“æœå¤±è´¥: {e}")
        
        return results
    
    def _extract_baidu_results(self, soup) -> List[Dict[str, str]]:
        """æå–ç™¾åº¦æœç´¢ç»“æœ"""
        results = []
        
        try:
            # æŸ¥æ‰¾æœç´¢ç»“æœå®¹å™¨
            search_results = soup.find_all('div', class_='result')
            
            for result in search_results[:5]:  # é™åˆ¶å‰5ä¸ªç»“æœ
                try:
                    # æå–æ ‡é¢˜
                    title_elem = result.find('h3')
                    title = title_elem.get_text().strip() if title_elem else ""
                    
                    # æå–é“¾æ¥
                    link_elem = result.find('a')
                    link = link_elem.get('href') if link_elem else ""
                    
                    # æå–æ‘˜è¦
                    snippet_elem = result.find('div', class_='c-abstract')
                    snippet = snippet_elem.get_text().strip() if snippet_elem else ""
                    
                    if title and link:
                        results.append({
                            'title': title,
                            'link': link,
                            'snippet': snippet
                        })
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ æå–ç™¾åº¦ç»“æœå¤±è´¥: {e}")
        
        return results
    
    def _extract_generic_results(self, soup) -> List[Dict[str, str]]:
        """æå–é€šç”¨æœç´¢ç»“æœ"""
        results = []
        
        try:
            # å°è¯•æŸ¥æ‰¾å¸¸è§çš„æœç´¢ç»“æœå…ƒç´ 
            title_selectors = ['h1', 'h2', 'h3', '.title', '.result-title']
            link_selectors = ['a[href]']
            snippet_selectors = ['p', '.snippet', '.description', '.summary']
            
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            links = soup.find_all('a', href=True)
            
            for link in links[:10]:  # é™åˆ¶å‰10ä¸ªé“¾æ¥
                try:
                    title = link.get_text().strip()
                    href = link.get('href')
                    
                    # è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥
                    if not title or not href or href.startswith('#') or 'javascript:' in href:
                        continue
                    
                    # æŸ¥æ‰¾ç›¸å…³çš„æ‘˜è¦
                    snippet = ""
                    parent = link.parent
                    if parent:
                        snippet_elem = parent.find(['p', 'div', 'span'])
                        if snippet_elem:
                            snippet = snippet_elem.get_text().strip()[:200]  # é™åˆ¶é•¿åº¦
                    
                    results.append({
                        'title': title,
                        'link': href,
                        'snippet': snippet
                    })
                    
                    if len(results) >= 5:  # é™åˆ¶ç»“æœæ•°é‡
                        break
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ æå–é€šç”¨ç»“æœå¤±è´¥: {e}")
        
        return results
    
    def get_search_summary(self, crawled_content: List[Dict[str, str]]) -> str:
        """
        ç”Ÿæˆæœç´¢ç»“æœæ‘˜è¦
        
        Args:
            crawled_content: çˆ¬å–çš„æœç´¢ç»“æœ
            
        Returns:
            æœç´¢ç»“æœæ‘˜è¦
        """
        if not crawled_content:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ"
        
        summary_parts = []
        summary_parts.append(f"æ‰¾åˆ° {len(crawled_content)} æ¡ç›¸å…³ç»“æœï¼š")
        
        for i, result in enumerate(crawled_content[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ
            title = result.get('title', '')
            snippet = result.get('snippet', '')
            
            summary_parts.append(f"\n{i}. {title}")
            if snippet:
                # é™åˆ¶æ‘˜è¦é•¿åº¦
                snippet_short = snippet[:100] + "..." if len(snippet) > 100 else snippet
                summary_parts.append(f"   {snippet_short}")
        
        return "\n".join(summary_parts)


# å…¨å±€æœç´¢æ¨¡å—å®ä¾‹
search_module = SearchModule() 